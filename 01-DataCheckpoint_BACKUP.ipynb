{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Megan Yu: Data Overview + Instruction questions for Dataset #1\n",
    "\n",
    "Olivia Huard: Instruction questions 1 and 2 for Dataset #2\n",
    "\n",
    "Shanmukhi Nandiraju: Data Overview + Instruction questions 3, 4, 5 for Dataset #2 + Project proposal section modifications\n",
    "\n",
    "Harshatha Prasanna: Data Overview + Instruction questions for Dataset #1\n",
    "\n",
    "Jazely Tong: Data Overview + Instruction questions for Dataset #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Proposal Fixes:\n",
    "\n",
    "Hi! Thank you so much for taking the time to go through our project proposal and give us feedback on it! Here are the changes we made to our project proposal to accomodate the suggestions (also included this message in a comment under the GitHub issue):\n",
    "\n",
    "-> To address the feedback on our Research Question, we specified that we will use a Random Forest model and listed the specific lexical features (length, entropy, and special characters) we will use for classification. We also moved our 90% accuracy target to the Hypothesis section.\n",
    "\n",
    "-> We expanded our Hypothesis to include a broader range of features, specifically character entropy and URL length, as suggested. We also incorporated our 90% accuracy goal into this section.\n",
    "\n",
    "-> We have fully discussed and completed our project timeline, adding specific tasks and goals for each team meeting, individual and group, and meeting dates to ensure we stay on track for the checkpoints and overall project.\n",
    "\n",
    "Please do let us know if there is anything else lacking. Thank you again!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we develop a Random Forest classification model to predict whether a URL is benign, phishing, malware or defacement based on lexical features such as URL length, character entropy, and the frequency of special characters? This predictive task aims to evaluate which combination of these structural attributes most effectively identifies maliciousness across various cybersecurity threat categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malicious URLs are a major vector for phishing, malware delivery, and website defacement, and attackers increasingly embed harmful intent in the structure of a URL itself. These URLs often imitate legitimate sites, mislead users into entering sensitive information, or redirect them to compromised resources. Recent studies highlight their growing prevalence: Mittal (2023) notes that anti-phishing systems confronted over half a billion phishing attempts in 2022, underscoring the persistence and escalation of URL-based attacks.<a name=\"cite1\"></a><a href=\"#ref1\">1</a> Similarly, Omolara and Alawida (2025) report that malicious links rose by 144% in a single year, driven by techniques such as social engineering, obfuscation, and automated URL generation.<a name=\"cite2\"></a><a href=\"#ref2\">2</a> Collectively, this research shows that understanding URL characteristics is essential for mitigating harms associated with phishing and related threats.\n",
    "\n",
    "Traditional blacklist-based defenses, such as browser-integrated URL safety checks, were among the earliest strategies for blocking malicious links. However, blacklist systems fundamentally depend on previously identified threats. As Omolara and Alawida explain, attackers frequently rotate domain names, use URL shorteners, manipulate HTTP/HTTPS presentation, or rely on fast-flux hosting, techniques that allow newly created malicious URLs to evade blacklist detection long enough to cause damage.<a href=\"#ref2\">2</a> This limitation has motivated a shift toward automated, feature-based analysis that can evaluate a URL’s structure without requiring prior knowledge of whether it is malicious.\n",
    "\n",
    "Prior work has analyzed the internal structure and lexical attributes of URLs to uncover which characteristics most reliably distinguish malicious links from benign ones. The literature identifies several strong predictors: the presence of an IP address instead of a domain name, abnormal anchor tags, unusually long URLs, excessive special characters, multi-subdomain patterns, and the use of misleading prefix/suffix tokens such as “-secure” or “-verify.” Mittal (2023) outlines dozens of such features, demonstrating how attributes like redirection counts, “@” symbols, URL entropy, and delimiter frequency can signal phishing activity.<a href=\"#ref1\">1</a> The recent survey by Tian et al. (2025) further categorizes these features into lexical, host-based, and content-based groups, emphasizing the predictive power of purely lexical features derived from characters, symbols, and token patterns within the URL string.<a name=\"cite3\"></a><a href=\"#ref3\">3</a> This aligns directly with our project’s focus on character-level URL analysis.\n",
    "\n",
    "In addition to feature engineering research, many studies have evaluated machine learning models for malicious URL classification. Mittal (2023) demonstrates that interpretable “glass box” models such as Logistic Regression and Decision Trees can achieve 90-95% accuracy using around 30 lexical and reputation-based features.<a href=\"#ref1\">1</a> More advanced approaches, such as the ensemble techniques evaluated in Omolara and Alawida’s DaE2 framework, reach up to 98% accuracy using boosting, bagging, and stacked models trained on large malicious-URL datasets.<a href=\"#ref2\">2</a> Tian et al. (2025) corroborate these findings, noting that character-level and token-level feature extraction remain among the strongest signals for ML-based URL detection.<a href=\"#ref3\">3</a> Together, this body of work establishes a strong foundation for predicting URL maliciousness using structural properties alone.\n",
    "\n",
    "Our project builds directly on these findings by focusing specifically on correlations between URL characters, symbols, and structural patterns, and whether those characteristics can be used to distinguish malicious URLs (e.g., phishing, malware, defacement) from benign ones. While prior studies have explored broad sets of lexical and host-based features, fewer have isolated the predictive value of character-level traits such as symbol frequency, delimiters, suspicious token patterns, and URL length variations. By quantifying these relationships and applying predictive modeling, our study aims to evaluate how well these standalone URL characteristics can determine URL authenticity and contribute to early malicious-URL detection.\n",
    "\n",
    "References  \n",
    "<a name=\"ref1\"></a>  \n",
    "Mittal, S. (2023). Explaining URL Phishing Detection by Glass Box Models. IC3 2023.  \n",
    "https://doi.org/10.1145/3607947.3608059  \n",
    "<a href=\"#cite1\">^</a>\n",
    "\n",
    "<a name=\"ref2\"></a>  \n",
    "Omolara, O. E., & Alawida, M. (2025). DaE2: Unmasking Malicious URLs via Consensus From Diverse Techniques.  \n",
    "https://doi.org/10.1016/j.cose.2024.104170  \n",
    "<a href=\"#cite2\">^</a>\n",
    "\n",
    "<a name=\"ref3\"></a>  \n",
    "Tian, Y., et al. (2025). From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets, and Code Repositories. arXiv:2504.16449. https://arxiv.org/abs/2504.16449  \n",
    "<a href=\"#cite3\">^</a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group predicts that URL's categorized as malicious will exhibit higher character entropy, much longer string lengths, and a higher frequency of special characters like '$', '@', '!', \"%\", etc.) compared to benign URLs. We also predict that a classification model trained on these lexical features will achieve at least 90% accuracy in distinguishing between threat categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1\n",
    "\n",
    "  - **Dataset Name**: Malicious URLs dataset\n",
    "  - **Link to the dataset**: https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset\n",
    "  - **Number of observations**: 651,191 total URLs are included in the dataset.\n",
    "  - **Number of variables**: The dataset contains 2 main variables: URLS and type.\n",
    "\n",
    "  - **Description of the variables most relevant to this project:**\n",
    "    - **url:** (string):This is the web address itself \n",
    "    - **type:** (categorical label): This indicates what kind of URL it is: \n",
    "      - benign: legitimate URL\n",
    "      - defacement: website appearance has been altered\n",
    "      - phishing: designed to trick users into giving private info\n",
    "  malware: URL leads to harmful downloads\n",
    "  - **Descriptions of any shortcomings this dataset has with respect to the project**\n",
    "  - This dataset has a limited number of variables and no given numerical features which will make it difficult when attempting to train the model.\n",
    "  - This dataset consists of 66% \"benign\" URLs. This imbalance can make training models challenging because most models may simply learn to predict “benign” due to the skew in the data.\n",
    "\n",
    "### Dataset 2\n",
    "\n",
    "- **Dataset Name:** URL-Phish: A Feature-Engineered Dataset for Phishing Detection\n",
    "- **Link to the dataset:** [https://data.mendeley.com/datasets/65z9twcx3r/1](https://data.mendeley.com/datasets/65z9twcx3r/1)\n",
    "- **Number of observations:** The dataset contains 111,660 unique URLs\n",
    "- **Number of variables:** There are 26 total columns. This includes 22 numerical feature columns, 3 string reference columns (url, dom, tld), and 1 binary label column\n",
    "- **Description of the variables most relevant to this project:**\n",
    "    - **label:** The target variable, where 0 represents benign and 1 represents phishing\n",
    "    - **url_len:** The total length of the URL in characters, which has a mean of 32.95 but reaches a maximum of 1,202\n",
    "    - **entropy:** A measure of the randomness of characters in the URL, ranging from 2.65 to 6.03 bits\n",
    "    - **digit_ratio:** The proportion of numerical digits relative to the total URL length. Most samples have a low ratio (mean 0.013), but some reach as high as 0.826\n",
    "    - **is_https:** A binary flag (0 or 1) indicating if the URL uses a secure connection; approximately 43.1% of the dataset uses HTTPS\n",
    "- **Descriptions of any shortcomings this dataset has with respect to the project:**\n",
    "    - The dataset is heavily skewed, with phishing samples making up only 14.2% of the data compared to 85.8% for benign samples. This imbalance can cause models to be biased toward predicting \"benign\" by default\n",
    "    - There will be a sampling bias in Benign data, because Benign URLs were sourced exclusively from \"trusted sources\" such as educational (.edu), governmental (.gov), and top-ranked domains. This may not accurately represent the full diversity of legitimate URLs across the broader internet\n",
    "    - There are also temporal limitations because the phishing samples were collected between November 2024 and September 2025 and since phishing tactics change constantly, the features may not be as good predictors anymore on the legitimacy of a site\n",
    "\n",
    "\n",
    "**Plan to combine these datasets:** By combining Datasets 1 and 2, we hope to combat the limited feature set in Dataset 1 by incorporating the additional numerical variables recorded in Dataset 2. Both datasets contain a variable that identifies whether the URL is benign or malicious, so they will be standardized for a common binary format following Dataset 2’s structure where 0 represents benign and 1 represents malicious. To combine them, the numerical features in Dataset 2 will be engineered for Dataset 1 to have both datasets share the same column structure before being aligned. We will also remove duplicated URLs as necessary to avoid any bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading airline-safety.csv:   0%|          | 0.00/1.23k [00:00<?, ?B/s]\u001b[A\n",
      "                                                                           \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: airline-safety.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading bad-drivers.csv:   0%|          | 0.00/1.37k [00:00<?, ?B/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████| 2/2 [00:00<00:00, 15.66it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: bad-drivers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "#%pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "] \n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "                \n",
    "                \n",
    "                                                \n",
    "                                                    Malicious URL Multi-Class Classification Dataset \n",
    "                \n",
    "This dataset contains 651,191 web URLs labeled according to cybersecurity threat category. Each row represents a single URL and a corresponding label in the type column. The dataset includes four classes: benign, phishing, malware, and defacement. Benign URLs represent legitimate websites. Phishing URLs are deceptive sites created to steal sensitive information such as login credentials. Malware URLs distribute harmful software. Defacement URLs refer to websites that have been compromised and altered by attackers.\n",
    "\n",
    "The important variables in this dataset are the url column and the type column. The url variable is a text string representing a web address. While URLs do not have physical measurement units like medical or financial data, they contain measurable structural characteristics. For example, URL length can be measured in number of characters, and unusually long URLs may indicate suspicious behavior such as obfuscation, excessive parameters, or encoded content. The type variable is categorical and represents the classification of each URL into one of the four threat categories. This variable serves as the outcome (target) variable for classification modeling.\n",
    "\n",
    "There are several important concerns with this dataset. First, the dataset was compiled from publicly available sources, meaning the selection of malicious URLs likely reflects previously detected or reported threats rather than newly emerging ones. This may introduce sampling bias. Second, the dataset is class imbalanced, with benign URLs forming the largest group. This imbalance may influence model performance if not properly addressed. Third, the labeling process depends on external threat intelligence sources, meaning some labels may contain errors or outdated classifications. Finally, because the dataset does not include timestamps, it does not account for how URL patterns and attack strategies change over time. The dataset is already in tidy format, as each row represents a single URL observation and each column represents a single variable.\n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/00-raw/malicious_urls.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#loading dataset \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/00-raw/malicious_urls.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#data snippet \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 5 rows of the dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/00-raw/malicious_urls.csv'"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "\n",
    "#loading dataset \n",
    "df1 = pd.read_csv(\"data/00-raw/malicious_urls.csv\") \n",
    "\n",
    "#data snippet \n",
    "print(\"First 5 rows of the dataset:\") \n",
    "display(df1.head()) \n",
    "\n",
    "# dataset information print(\"\\nDataset info:\") \n",
    "df1.info() \n",
    "\n",
    "# dataset size \n",
    "print(\"\\nDataset size:\") \n",
    "print(\"Rows:\", df1.shape[0]) \n",
    "print(\"Columns:\", df1.shape[1])\n",
    "\n",
    "# check for missing data, there are no missing values \n",
    "print(\"\\nMissing Data By Col:\") \n",
    "print(df1.isnull().sum()) \n",
    "\n",
    "\n",
    "# getting rid of duplicates \n",
    "df1 = df1.drop_duplicates() \n",
    "\n",
    "# clean label column \n",
    "df1['type'] = df1['type'].str.lower().str.strip() \n",
    "\n",
    "# add URL length as a new feature \n",
    "df1['url_length'] = df1['url'].str.len() \n",
    "\n",
    "\n",
    "# extremely long URLs are potential outliers (~0.004%) , we will keep these outliers as they will help us determine commonalities between benign and non-benign urls outliers = df1[df1['url_length'] > 1000] print(\"\\nNumber of suspiciously long URLs (>1000 chars):\", outliers.shape[0]) \n",
    "df1['very_long_url'] = df1['url_length'] > 1000 \n",
    "# summary statistics print(\"\\nURL length summary statistics:\") \n",
    "print(df1['url_length'].describe()) \n",
    "print(\"\\nCounts per URL type:\") \n",
    "print(df1['type'].value_counts()) \n",
    "df1.head() \n",
    "\n",
    "#saving to cleaned dataset to processed folder \n",
    "processed_path = \"data/02-processed/malicious_urls_clean.csv\" \n",
    "os.makedirs(os.path.dirname(processed_path), exist_ok=True) \n",
    "df1.to_csv(processed_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset #2:\n",
    "\n",
    "\n",
    "                            Mix of Benign and Phishing URLS: Specified Characters, and Measures of Characters\n",
    "\n",
    "\n",
    "\tThe most important metrics in this dataset to answer our research question include, the counts of different characters, the label, url length, entropy, seeing if it uses https, and digit ratio. Most of these different metrics are in integer form, specifically counts of different characters, seeing if it uses https, the label, and url length. The only ones which are different are entropy and digit ratio, which are both floats.\n",
    "\n",
    "For the counts of different characters, there are no specific unit types involved, only different numbers quantifying how many characters there are in a specific URL. For example, if a URL is “yay12_” then the character count of _ would go up by one. This will be beneficial to see if different characters influence whether or not a URL is benign or phishing.\n",
    "\n",
    "For the label, there are no specific unit types involved either. The values for label are either 0 or one. The section label is essentially just letting the person reading the dataset know if a specific URL is benign or phishing, with a 0 or one representing each respectively.\n",
    "\n",
    "For the URL length, this is again not using any specific unit types, just ranging numbers, quantifying how many characters make up any given URL. For instance if a URL is “fake_url” then the URL length would have the number 8, the number of characters in a URL. This will be helpful to see if benign URLS are shorter or longer on average than phishing URLS.\n",
    "\n",
    "Entropy talks about how random any given character is in a URL. This is essentially talking about the average of randomness in a URL in our specific dataset. This is measured by a float, and can range in value if a URL is more random than another. This is so we can see if benign URLS are more or less random than phishing URLS.\n",
    "\n",
    "Digit ratio in this dataset is how many numbers a specific URL has compared to other characters in it. This is a float, or decimal, that can have quite a range in value depending on the different URL being looked at. This will allow us to look and see if benign URLS use more numbers on average than a phishing URL.\n",
    "\n",
    "Finally, seeing if a URL uses HTTPS or not. This will either be a value of 0 or one, which shows that a specific URL is using HTTPS(0) or not(1). This will allow us to see if HTTPS has any impact on the legitimacy of a URL.\n",
    "\n",
    "\tThe major concern with this dataset is the fact that it is mostly made up of benign URLS. This dataset being made up of mostly benign URLS, makes it so we will have less overall data on malicious URLS. This means the model our group ends up making in the end, will be quite good at predicting if a URL is benign, but could be less adept at pinpointing if the URL we give it is specifically phishing. The other part of our research question, seeing if a URL is malware, or its purpose is defacement, will also not be able to be answered specifically with this dataset. To answer that, the model will require the usage of the other set to fill in the blind spot this dataset has. This set is also a bit old for looking into how to best predict a phishing URL, which could cause it to work better on URLS from the timeframe the ones in the dataset are collected from. This makes it so as time goes on, the model will become less and less accurate with predicting whether or not a URL is benign or phishing, unless accurate, and up to date data is given to the model, so it can continue learning. These are specific issues of the dataset to consider, as we attempt to answer our research question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# loading the raw dataset\n",
    "df = pd.read_csv('data/00-raw/URL_Phish_FeatureSet.csv')\n",
    "\n",
    "# demonstrating size and tidiness\n",
    "# --> the dataset is tidy because every column is a lexical variable and every row is a unique url\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head() \n",
    "\n",
    "# finding missing data and assessing systematic bias\n",
    "# --> if nulls are rare and scattered, they are likely missing at random\n",
    "null_values = df.isnull().sum()\n",
    "print(\"Missing Values per Column:\\n\", null_values)\n",
    "\n",
    "# finding and flagging outliers (> 3 sd from mean)\n",
    "stats = df.describe()\n",
    "print(stats)\n",
    "\n",
    "# cleaning the data with dropna() because with 111,660 samples, dropping a few rows will not hurt\n",
    "processed_df = df.dropna(how='any').copy()\n",
    "\n",
    "# final wrangling: dropping string reference columns (url, dom, tld)\n",
    "# --> because these are preserved for interpretability but not used for model training\n",
    "processed_df = processed_df.drop(columns=['url', 'dom', 'tld'])\n",
    "\n",
    "# writing the fully wrangled version of the data to data/02-processed\n",
    "processed_df.to_csv('data/02-processed/URL_Phish_Final.csv', index=False)\n",
    "\n",
    "# summary statistics for target variable distribution (0 = benign, 1 = phishing)\n",
    "print(\"Final Label Distribution:\\n\", processed_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is just to learn some more about the dataset!\n",
    "\n",
    "df2 = pd.read_csv('data/02-processed/URL_Phish_Final.csv')\n",
    "\n",
    "print(\"feature columns:\", df2.columns.tolist())\n",
    "\n",
    "print(\"\\nsummary statistics:\\n\", df2.describe())\n",
    "\n",
    "print(\"\\nclass distribution:\\n\", df2['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\ndata types:\\n\", df2.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> Most of the datasets we are currently looking at are publicly available datasets, so the people whose URL's are included have not consented for their data to be used in our specific project.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "> The datasets we are looking at currently were published between 1 to 5 years ago, so we have temporal bias (unless we find more recent datasets to pull from), since if we train our model on older data it may fail to recognize more recent/modern threats or patterns because it would prioritize looking for characteristsics that are outgrown/outdated.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "> Yes, our project specifically needs to focus on this ethical concern because URL's can frequently contain PII like emails or user IDs. We'd need to consider anonymizing these URL's or stripping these types of parameters to avoid exposing any user data.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "> We've considered that malicious URLs may be more frequently associated with certain regions or languages, and that theres a possibility our model could unfairly flag benign sites if they have structural similarities with known malicious domains, which is something we should try to minimize if possible.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "\n",
    "> Yes, we realize that storing a large database of malicious URLs could be a security risk if its accessed by people with intentions to study existing malware or learn from it and cause more attacks, so we plan to keep the data secured within the repo and datahub.\n",
    "\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "\n",
    "> We've considered this yes, but at the same time since we are using third-party datasets we dont really have a way to remove someone's URL upon request. We don't know if there is a work around for this ethical concern in our project's case.\n",
    "\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "> Yes, we will delete the data once we finish the project and the quarter ends\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    "> Yes, we think one main blindspot is that we arent cybersecurity experts, so there is a lot of perspective that we would lack when it comes to understaning certain aspects of phishing. The main thing we could attempt to do to fix this is by researching/learning more about it before we start.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    "> We talked about how the model may develop a confirmatio bias if the training data is imbalanced, like for example if we use more malware URLs to train it than defacement URLs. So the steps we'd need to take to mitigate these possible biases is to make sure the model doesnt just \"learn\" to guess the most common class.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "\n",
    "> Yes, our goal for our representations will be that they clearly define what makes up a malicious URL as best as we can and as clearly as we can to avoid misleading audiences in any way.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "\n",
    "> We have considered this, and we will ensure that raw URLs containing any PII are not displayed in sny representations nor the final report.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "> Yes, all of our process will be documented in Jupyter Notebooks and pushed to our GitHub repo to make sure the results are reproducible and accessible if we discover any issues in the future.\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "\n",
    "> Yes, we acknowledge this, and we will research the variables thoroughly enough to ensure they aren't acting as proxies for discrimination against legitimate websites.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    "\n",
    "> We will thoroughly test for disparate error rates, e.g. if the model is way more accurate for \".com\" domains and way less accurate for \".org\" domains then the model is not fair, and we will either try to address this bias or address that failure in the final report.\n",
    "\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "\n",
    "> We've set a goal of 90% accuracy, but we also realize the cost of false positives (blocking a benign site) versus false negatives (letting malware pass through undetected), since a model with 90% accuracy that blocks 10% of safe sites across the world would be a big problem realistically.\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "\n",
    "> Yes we can, we can be transparent with our features and highlight which one triggered the URL to be marked as malicious, and based on that we can provide a technical justification in understandable terms.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "> Yes, in the final report and in any other documentations where its important to mention the shortcomings, we will make it clear that it is a class project and not a legitimate security tool in any way.\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "\n",
    "> No, we do not have a long-term deployment plan after the final project is completed.\n",
    "\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    "> Yes, we would need a plan for when our model wrongly labels legitimate sites as malicious, and we discussed ways which we could go about this, like for example proactively updating the model as the digital landscape changes or if we see a way to improve the model to avoid harmful results. \n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "\n",
    "> Yes, we agreed our model should have a defined mechanism to turn off or roll back the model if it starts to cause unindended harm or very incorrect results.\n",
    "\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    "> Yes, as mentioned before, we realize that if the model is used in a negative way to study the algorithm, or use it to test someone's malicious URLs so that they could maybe bypass our 90% accuracy, that our project could potentially become a tool to create better scams. So we acknowledge that if this were to be deployed we would need some sort of monitoring plan to prevent such actions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the team expectations that the team agrees to\n",
    "\n",
    "* If it's directed at a part of the project you're working on, make sure to respond to the group messages within 24 hours.\n",
    "* Make sure to be open minded when hearing others ideas\n",
    "* Messaging about the project will be conducted in the groupchat\n",
    "* Making sure that the feedback we give to others is kind, and is not overall negative. Making sure to phrase things in a non-judgemental way\n",
    "* We will have a rough meeting day and time every week, but we'll send out a WhenToMeet form weekly to see if that slot works best for a given week, or if a better day/time in a week works best to meet\n",
    "* We will communicate decisions that are important for the full group to know. If it's something less important, say we're working on a section with another member, and we're changing up a paragraph, then that doesn't necessarily have to be mentioned, unless the members making the changes want to share.\n",
    "* Everyone will do a mix of everything. We will slput up different things to work on, making an effort to make sure the work is evenly split up.\n",
    "* We will check in every week to see if people need support in doing a task, and to see what tasks need to get done for the week.\n",
    "* If we get off task and are working together, and are being noisy, please no shushing, instead say something like \"let's lock in guys.\" If we get off task and need to get back on \"let's lock in\" will be the phrase we use to signal to the whole team(or ourselves)\n",
    "* We will make a list of the tasks to do per week on the shared google doc. We will go over it every week together, see what's been completed, and how to move forward on the project for the upcoming week to make sure we're all on the same page\n",
    "* If someone is struggling on their section, they can reach out as soon as they would like to chat it out so we can help as a group, as that can make work go faster.\n",
    "* The person struggling should text\n",
    "* If someone knows they won't have something done by a deadline we set, they can communicate that without fear of judgement. We will work as a team to see what needs to get done, and help if needed. If it takes an extra day, or more time than expected, keep open communication with the group, and tell us if you need help!\n",
    "* If things get rough, keep an optimistic/not harshly negative attitude about the project during meetings\n",
    "* If there's conflict, then there can be multiple things we do:\n",
    "\n",
    "            - if comfortable, communicate with the other person directly, keep this cordial and open to hearing the other. Make sure to let the other say what they would like to, and truly hear what each other is saying. We don't have to become best friends (while that would be awesome), but we do have to work together toward a common goal. If it's a truly bad argument, see if you and the other person can work toward neutral, if not friends, as we do all have the same goal\n",
    "\n",
    "            -if unsure about how to approach someone with conflict, can go see Olivia, as she knows many different conflict tequniqes, and has a camp counsler book that can give advice. She can help hold a conversation like a restoritive circle if people are unsure about how to communicate between themselves and want help. The two who are in conflict can ask anyone to sit in on a convorsation to make sure that both voices are being heard, or if they want extra support\n",
    "\n",
    "            - in the event everything goes terribly and there's a big fallout we can't work toward neutral with, go to the prof\n",
    "\n",
    "* Overall, make sure to communicate with the group, be understanding of others, open to hearing others ideas, if there's conflcit making sure to deal with it in a healthy way, meet weekly, and use the group Google Doc to divy out work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing a machine learning model. Due to that, we will make sure to keep good communication between members, and ask for suport when needed.\n",
    "\n",
    "We will send out a WhenToMeet form weekley to check and see if the days chosen still work. If they don't, then we will adjust to a different day/time early in the week so we can make sure to be on the same page for the week moving forward.\n",
    "\n",
    "We will meet on zoom, or in person depending on people's avalibilities.\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/18  |  12:00-1:00 PM | Finalize datasets to use, divide up work for the checkpoint | Observe the datasets, note down important information to use for the checkpoint, and clean/tidy the datasets, and finish the data and data overview sections. Make sure Data Checkpoint is complete, Due Tonight | \n",
    "| 2/23-25  |  Individual work, but will meet on Wednesday from 12:00-1:00 PM to check in with eachother | Write Python functions to extract lexical features like URL length, character entropy, and frequency of special characters ($, !, @, %, etc). Start initial Exploratory Data Analysis (EDA) using histograms to check feature distributions | Discuss progress and questions if any |\n",
    "| 3/2-4  | Individual work, but will meet on Wednesday from 12:00-1:00 PM to check in with eachother | Create box plots to compare feature frequencies across categories (Malware vs. Benign). Fit the initial Random Forest model. Check for overfitting by comparing training vs. validation accuracy | Discuss progress, questions, and fill in the jupyter notebook for the EDA Checkpoint for submission |\n",
    "| 2/9-11  | Individual work, but will meet on Wednesday from 12:00-1:00 PM to check in with eachother | Fine-tune model parameters to reach the 90% accuracy goal. Perform a final check of the \"Condition Number\" to ensure features aren't redundant | Draft the \"Limitations\" section regarding cybersecurity expertise, address any questions, concerns, modifications needed, and plan any additional meetings we would need to have |\n",
    "| 3/16-18  | Individual work on the Final Report (once we divide the work up), but will meet on Wednesday from 12:00-1:00 PM to record the presentation video and to wrap up any ends | Complete the technical Final Report | Record the 3-5 minute video presentation intended for a non-technical audience, and submit individual Team Evaluation surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
